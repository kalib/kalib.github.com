<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="https://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Kubernetes | Marcelo Cavalcante Rocha ~]]></title>
  <link href="https://kalib.github.io/blog/categories/kubernetes/atom.xml" rel="self"/>
  <link href="https://kalib.github.io/"/>
  <updated>2018-11-24T18:11:03-05:00</updated>
  <id>https://kalib.github.io/</id>
  <author>
    <name><![CDATA[Marcelo Cavalcante Rocha - Kalib]]></name>
    
  </author>
  <generator uri="https://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Terraform: Criando uma infraestrutura no Google Cloud]]></title>
    <link href="https://kalib.github.io/blog/2018/11/24/terraform-criando-uma-infraestrutura-no-google-cloud/"/>
    <updated>2018-11-24T12:39:00-05:00</updated>
    <id>https://kalib.github.io/blog/2018/11/24/terraform-criando-uma-infraestrutura-no-google-cloud</id>
    <content type="html"><![CDATA[<p><img class="center" src="/imgs/terraform_cloud.png" title="&lsquo;Terraform &ndash; Cloud Computing&rsquo;" ></p>

<p><strong>Q</strong>uando se fala em infraestrutura como código imagina-se algo mais complexo do que simplesmente um container docker rodando uma aplicação, certo?! O propósito deste post é justamente criar uma infraestrutura um pouco mais complexa e completa no GCP, <em>Google Cloud Platform</em>. Embora o Terraform possua integração com diversos provedores de computação em nuvem, utilizarei o Google Cloud para este post por estar trabalhando mais com GCP atualmente e estar gostando da experiência.</p>

<p><strong>U</strong>ma vez que não entrarei em tantos detalhes básicos do Terraform neste post, espero que você já possua algum conhecimento básico sobre o mesmo bem como entenda como funcionam seus <em>resources</em>, <em>variables</em>, etc. Do contrário, recomendo <strong>fortemente</strong> que você volte um pouco e leia meus posts anteriores nesta respectiva ordem:</p>

<ul>
<li><a href="/blog/2018/10/22/infraestrutura-como-codigo-com-terraform/">Infraestrutura como Código com Terraform</a></li>
<li><a href="/blog/2018/10/29/introducao-ao-terraform/">Introdução ao Terraform</a></li>
<li><a href="/blog/2018/11/06/terraform-variaveis-e-outputs/">Terraform: Variáveis e Outputs</a></li>
</ul>


<p><strong>A</strong>ssumindo que você já possui algum conhecimento básico sobre Terraform, chegou a hora de inciarmos o nosso pequeno projeto de infraestrutura como código.</p>

<h2>GCP &ndash; Google Cloud Platform</h2>

<p><img class="center" src="/imgs/gcloud.png" title="&lsquo;Google Cloud Platform&rsquo;" ></p>

<p><strong>G</strong>oogle Cloud Platform, ou GCP, é uma suíte de computação em nuvem oferecida pelo Google, funcionando na mesma infraestrutura que a empresa utiliza para seus produtos dirigidos aos usuários, dentre eles o buscador Google e o Youtube. Juntamente com um conjunto de ferramentas de gerenciamento modulares, o GCP fornece uma série de serviços incluindo computação, armazenamento de dados, análise de dados, machine learning, containers, etc.</p>

<p><strong>N</strong>ovamente, o motivo pelo qual optei por utilizar GCP para este pequeno projeto foi simplesmente o fato de eu estar trabalhando mais com GCP em meu dia a dia atual, mas nada impede que você utilize AWS ou Microsoft Azure, por exemplo. Embora a sintaxe e o código Terraform deverá ser ajustado para tais plataformas caso decida utilizá-las.</p>

<p><strong>O</strong>utro motivo interessante é o plano gratuito oferecido pelo Google, o que facilita nossos estudos e experimentos. O GCP nos oferece 1 ano de utilização grátis OU $300 dólares em créditos, o que ocorrer primeiro. De uma forma ou de outra, isto será muito mais que o suficiente para a execução deste nosso projeto.</p>

<p><strong>A</strong>lém das duas razões já citadas, a integração e facilidade de criação de uma nova conta no GCP foi levada em conta para esta escolha. O fato de o gmail e demais serviços do Google serem amplamente utilizados, é bem provável que você possua um email do Google (gmail, por exemplo), certo!? Se este é o caso, você já possui uma &ldquo;pré-conta&rdquo; no GCP sem ao menos saber.</p>

<h3>Cadastro</h3>

<p><strong>S</strong>e você possui uma conta do Google, autentique-se com a mesma. Caso não possua uma, você poderá criar uma através do Gmail, por exemplo, ou criar diretamente na interface do Google Cloud durante o cadastro.</p>

<p>1-   Uma vez logado com sua conta do Google, acesse em seu navegador o seguinte endereço: <a href="https://cloud.google.com">https://cloud.google.com</a> ;</p>

<p>2-   Caso esteja logado com sua conta do Google, verá sua foto ou imagem de conta no canto superior direito da página conforme na imagem a seguir. Clique em <em>Try free</em> ou Experimente Gratuitamente;</p>

<p><img class="center" src="/imgs/gcloud1.png" title="&lsquo;Google Cloud Platform&rsquo;" ></p>

<p>3-   Você cairá na primeira página do cadastro. Informe seu País, leia e concorde com os termos de uso caso deseje seguir e em seguida clique em Concordar e Continuar;</p>

<p><img class="center" src="/imgs/gcloud2.png" title="&lsquo;Google Cloud Platform&rsquo;" ></p>

<p>4-   Após confirmar e validar todas as informações que eles pedem na etapa 2, clique em Iniciar minha avaliação gratuita;</p>

<p><img class="center" src="/imgs/gcloud3.png" title="&lsquo;Google Cloud Platform&rsquo;" ></p>

<p><strong>E</strong>m poucos segundos você deverá cair no painel ou <em>dashboard</em> principal do GCP, com um popup de boas vindas com alguma mensagem de boas vindas: &ldquo;Olá, Marcelo, Agradecemos por você se inscrever na avaliação gratuita de 12 meses. Demos US$ 300 de crédito grátis para você gastar. Não se preocupe se o crédito acabar, você só receberá cobranças se tivermos sua permissão.&rdquo;</p>

<p><strong>E</strong>ste é outro fator interessante do GCP. Durante este período de avaliação, você não corre o risco de ser cobrado caso utilize mais do que deveria por descuido. Uma vez que o período de 12 meses, ou o crédito de $300 tenha se esgotado, você será notificado e deverá encerrar sua conta ou confirmar que deseja continuar utilizando os serviços, autorizando assim o Google a lhe efetuar cobranças a partir deste momento.</p>

<p><strong>O</strong> painel principal se parecerá com este:</p>

<p><img class="center" src="/imgs/gcloud4.png" title="&lsquo;Google Cloud Platform&rsquo;" ></p>

<p><strong>C</strong>onforme a imagem abaixo, através do Menu principal que se encontra na lateral esquerda, clique em <strong>IAM e Admin</strong>, e em seguida em <strong>Gerenciar recursos</strong>.</p>

<p><img class="center" src="/imgs/gcloud5.png" title="&lsquo;Google Cloud Platform&rsquo;" ></p>

<p><strong>V</strong>ocê receberá uma listagem inicial de seus recursos. Por padrão, quando se cria uma nova conta no GCP, apenas um projeto inicial é criado, chamado My First Project, sem qualquer recurso vinculado ou inserido no mesmo.</p>

<h3>Criando uma conta de serviço ou service account</h3>

<p>Embora seja possível e permitido utilizar-se de uma conta pessoal para este tipo de tarefa, não é o mais indicado. O ideal é deixar serviços utilizarem contas específicas, as chamadas service accounts. Uma vez que utilizaremos o Terraform para criar nossos recursos na nuvem, ele não deixa de funcionar como um serviço, não sendo uma pessoa XYZ de um departamento qualquer de uma empresa.</p>

<p>1-   No menu principal da lateral esquerda, clique em <strong>IAM e Admin</strong>, em seguida em <strong>Contas de serviço</strong>, conforme na imagem abaixo:</p>

<p><img class="center" src="/imgs/gcloud7.png" title="&lsquo;Google Cloud Platform&rsquo;" ></p>

<p>2-   Na tela de Contas de serviço, clique em <strong>Criar conta de serviço</strong>;</p>

<p>3-   Indique o nome <strong>terraform</strong> para esta conta, conforme imagem abaixo, e clique em <strong>CRIAR</strong>;</p>

<p><img class="center" src="/imgs/gcloud8.png" title="&lsquo;Google Cloud Platform&rsquo;" ></p>

<p>4-   No passo 2, daremos um papel para esta conta de serviço. Aqui indicaremos quais permissões ela terá. Para facilitar nosso exercício, utilizaremos <strong>Projeto</strong> > <strong>Proprietário</strong>, significando que nossa conta de serviço terá todas as permissões em nosso projeto, podendo criar ou destruir qualquer tipo de recurso.</p>

<p><img class="center" src="/imgs/gcloud9.png" title="&lsquo;Google Cloud Platform&rsquo;" ></p>

<p>5-   Em seguida, clicaremos em <strong>Continuar</strong> para seguir com a criação de nossa conta de serviço;</p>

<p>6-   Na tela seguinte lhe será dada a opção de dar permissões para algum usuário que possa precisar utilizar-se desta conta de serviço para criar recursos. Você pode inserir neste campo o seu usuário principal do google cloud, o qual será o seu email que foi utilizado para criar esta conta no GCP. Insira-o em <em>Papel de usuários da conta de serviço</em>. Em seguida, clique no botão de <em>Criar Chave</em> que se encontra logo abaixo. Esta será a chave criptografada que utilizaremos para comunicação segura com o GCP;</p>

<p><img class="center" src="/imgs/gcloud10.png" title="&lsquo;Google Cloud Platform&rsquo;" ></p>

<p>7-   Lhe será perguntado em que formato você deseja slavar a chave. Escolha o formato JSON e clique em <strong>CRIAR</strong>;</p>

<p>8-   Escolha com atenção o local onde salvará esta chave, pois você não poderá baixá-la novamente e precisaremos da mesma posteriormente;</p>

<p><strong>OBS:</strong> Para facilitar este exemplo, estarei salvando a chave no mesmo diretório no qual criaremos o código de nosso projeto. Caso você decida fazer o mesmo, e decida hospedar este código em alguma espécie de repositório git, por exemplo, lembre-se de <strong>sempre</strong> incluir esta e outras chaves ou dados sigilosos em seu <em>.gitignore</em>, para que este tipo de arquivo com informações confidenciais não seja enviado para o repositório juntamente com o código. =) (Sim, já vi pessoas hospedando chaves em repositórios git e tendo sérios problemas. Sim, é você mesmo. :p)</p>

<h3>Google Cloud SDK</h3>

<p><strong>U</strong>m dos nossos pré-requisitos será o Google Cloud SDK, que possui um conjunto de ferramentas via linha de comando que nos permitem interagir com o Google Cloud remotamente através de APIs. O Terraform também fará uso desta ferramenta.</p>

<p><strong>P</strong>ara o funcionamento do gcloud SDK precisaremos também possuir o Python instalado, na versão 2.7. Você poderá confirmar a sua versão do python executando <em>python -V</em> em algum console ou terminal.</p>

<p><strong>I</strong>nformações detalhadas sobre como instalar o Gcloud SDK encontram-se com excelentes detalhes nas páginas oficiais do Google, listadas abaixo:</p>

<ul>
<li><p>Para Linux (genérico): <a href="https://cloud.google.com/sdk/docs/quickstart-linux">https://cloud.google.com/sdk/docs/quickstart-linux</a></p></li>
<li><p>Para Linux (Debian e Ubuntu): <a href="https://cloud.google.com/sdk/docs/quickstart-debian-ubuntu">https://cloud.google.com/sdk/docs/quickstart-debian-ubuntu</a></p></li>
<li><p>Para Linux (Red Hat e CentOS): <a href="https://cloud.google.com/sdk/docs/quickstart-redhat-centos">https://cloud.google.com/sdk/docs/quickstart-redhat-centos</a></p></li>
<li><p>Para Mac OS X: <a href="https://cloud.google.com/sdk/docs/quickstart-macos">https://cloud.google.com/sdk/docs/quickstart-macos</a></p></li>
<li><p>Para Windows: <a href="https://cloud.google.com/sdk/docs/quickstart-windows">https://cloud.google.com/sdk/docs/quickstart-windows</a></p></li>
</ul>


<p><strong>É</strong> importante lembrar de reiniciar o seu terminal ou console após a instalação.</p>

<p><strong>P</strong>ara certificar-se de que a instalação foi realizada com sucesso, execute:</p>

<p>``` yaml
$ gcloud &mdash;version</p>

<p>Google Cloud SDK 225.0.0
bq 2.0.37
core 2018.11.09
gsutil 4.34
```</p>

<p><strong>S</strong>e você recebeu informações referentes à versão do Google Cloud SDK, significa que a instalação foi bem sucedida. O próximo passo é autenticar-se com sua conta do google através do SDK. Execute <em>gcloud init</em>. O seu navegador deverá abrir automaticamente lhe pedindo a autenticação de sua conta do Google após você confirmar com um <em>Y</em> a solicitação no console ou terminal.</p>

<p>``` yaml
$ gcloud init</p>

<p>Welcome! This command will take you through the configuration of gcloud.</p>

<p>Your current configuration has been set to: [default]</p>

<p>You can skip diagnostics next time by using the following flag:
  gcloud init &mdash;skip-diagnostics</p>

<p>Network diagnostic detects and fixes local network connection issues.
Checking network connection&hellip;done.
Reachability Check passed.
Network diagnostic passed (1/1 checks passed).</p>

<p>You must log in to continue. Would you like to log in (Y/n)?
```</p>

<p><strong>C</strong>aso ao inserir um <em>Y</em>, o seu navegador não lhe solicite a autenticação do Google, copie e cole a longa URL que lhe será apresentada.</p>

<p><strong>A</strong> solicitação de autenticação será similar à esta:</p>

<p><img class="center" src="/imgs/gcloud6.png" title="&lsquo;Google Cloud Platform&rsquo;" ></p>

<p><strong>A</strong>o finalizar sua autenticação no navegador, você receberá mais uma pergunta em seu terminal ou console. O SDK lhe indicará o seu projeto e lhe perguntará se você quer utilizá-lo ou criar um novo. Vamos escolher a opção <em>1</em>, para utilizar o projeto que foi criado automaticamente e em seguida confirmar com um Enter:</p>

<p>``` yaml
Your browser has been opened to visit:</p>

<pre><code>https://accounts.google.com/o/oauth2/auth?reblablabalblablaba=http%3A%2F%2Flocalhost%3A8085%2F&amp;prompt=select_account&amp;response_typereblablabalblablabauid.apps.googleusercontent.com&amp;scope=https%3A%2F%2Fwww.googleapis.com%reblablabalblablabaemreblablabalblablabaw.gooreblablabalblablaba%2Fauth%2Fclreblablabalblablaba%2Fwwwreblablabalblablabacomreblablabalblablabaappenreblablabalblablabattpsreblablabalblablabaww.reblablabalblablabaleapis.com%2Fauthreblablabalblablabattps%3A%2F%2Fwww.googleapis.com%2Fauthreblablabalblablaba&amp;access_type=offline
</code></pre>

<p>You are logged in as: [<a href="&#x6d;&#x61;&#x69;&#x6c;&#116;&#111;&#58;&#x6d;&#97;&#114;&#99;&#x65;&#x6c;&#111;&#64;&#x6d;&#x61;&#x72;&#99;&#x65;&#108;&#111;&#101;&#x6d;&#x61;&#105;&#108;&#46;&#110;&#x65;&#116;">&#x6d;&#x61;&#114;&#99;&#x65;&#108;&#111;&#64;&#x6d;&#97;&#114;&#99;&#101;&#x6c;&#111;&#x65;&#109;&#x61;&#x69;&#x6c;&#x2e;&#x6e;&#101;&#116;</a>].</p>

<p>Pick cloud project to use:
 [1] possible-sun-meuid
 [2] Create a new project
Please enter numeric choice or text value (must exactly match list
item):
```</p>

<p><strong>O</strong> SDK finalizará o setup e lhe informará que o projeto está pronto para ser utilizado.</p>

<h2>Terraform</h2>

<p><img class="center" src="/imgs/terraform_badge.png" title="&lsquo;Terraform&rsquo;" ></p>

<p><strong>V</strong>amos ao que interessa agora. Antes de iniciarmos nosso código, crie um diretório chamado <em>terraform-gcp</em>, ou algo de sua preferência. Dentro deste diretório apenas teremos por enquanto o arquivo JSON que baixamos do GCP com as credenciais de nossa conta de serviço.</p>

<p><strong>C</strong>riaremos agora nosso primeiro arquivo terraform.</p>

<p><strong>C</strong>omecemos criando nosso arquivo de variáveis, o qual por enquanto conterá apenas 2 variáveis para começarmos nosso código. Crie o arquivo <em>variables.tf</em> com o seguinte conteúdo:</p>

<p>``` py variables.tf
variable &ldquo;project_id&rdquo; {
  type    = &ldquo;string&rdquo;
  default = &ldquo;possible-sun-83482736&rdquo;
}</p>

<p>variable &ldquo;regiao&rdquo; {
  type = &ldquo;string&rdquo;
  default = &ldquo;northamerica-northeast1&rdquo;
}
```</p>

<p><strong>OBS:</strong> Lembre-se de alterar o valor default da variável <em>project_id</em>. Eu inseri aqui o id do projeto que foi criado para mim pelo GCP automaticamente. O id do seu projeto deverá ser o mesmo fornecido a você pelo GCP.</p>

<p><strong>O</strong>utra coisa importante é lembrar que estou assumindo que você já possui algum conhecimento básico sobre como o Terraform funciona, ou que leu meus posts anteriores sobre o assunto, de forma que não estarei aqui entrando em tantos detalhes explicativos sobre cada arquivo, conforme fiz nos anteriores.</p>

<p><strong>E</strong>m nosso arquivo <em>variables.tf</em>, por enquanto, criamos apenas duas variáveis: <em>project_id</em> e <em>regiao</em>. Para quem já utilizou algum serviço de computação em nuvem, seja GCP, AWS, Azure, etc., isto pode soar familiar. Sempre que se deseja criar recursos na nuvem, devemos optar por alguma região disponível no provedor de escolha. Estou optando por utilizar <em>northamerica-northeast1</em> pelo fato de eu morar em Toronto, e esta ser a região mais próxima, mas sinta-se livre para optar por qualquer região disponível no GCP conforme <a href="https://cloud.google.com/compute/docs/regions-zones/">lista fornecida aqui</a>.</p>

<p><strong>E</strong>m seguida criaremos nosso arquivo <em>main.tf</em> que, inicialmente, possuirá apenas o seguinte:</p>

<p>``` py main.tf</p>

<h1>Configura o projeto GCP</h1>

<p>provider &ldquo;google&rdquo; {
  credentials = &ldquo;${file("possible-sun-83482736-sabh45jhb2345ghv.json&rdquo;)}&ldquo;
  project     = &rdquo;${var.project_id}&ldquo;
  region      = &rdquo;${var.regiao}&ldquo;
}
```</p>

<p><strong>A</strong>qui estamos apenas informando ao Terraform que utilizaremos o <em>google</em> como provedor ou <em>provider</em>. Para nos conectarmos com o <em>provider</em> precisamos passar nossas credenciais e para tal estamos apontando o arquivo ou <em>file</em> que baixamos do GCP com as informações de nossa conta de serviço. Aqui estou passando apenas o nome do arquivo, pois o mesmo se encontra no mesmo diretório onde se encontra meu código.</p>

<p><strong>A</strong>lém da credencial, estamos também informando qual o nome do projeto que utilizaremos no GCP bem como a região na qual estaremos trabalhando. Ambos os valores estão sendo trazidos do arquivo <em>variables.tf</em>. Aqui apenas invocamos as variáveis. (Novamente: Se esta invocação das variáveis lhe parece confusa, significa que não leu o post anterior, onde expliquei o básico sobre uso de variáveis no Terraform. Volte uma casa!)</p>

<p><strong>C</strong>om ambos os arquivos criados, podemos iniciar a execução de nosso projeto. Neste momento, nosso código não fará nada além de permitir que o Terraform consiga se conectar ao GCP e validar que nossa conta e projeto de fato existem. Executemos <em>terraform init</em> para ver se está tudo certo:</p>

<p>``` yaml
$ terraform init</p>

<p>Initializing provider plugins&hellip;
&ndash; Checking for available provider plugins on <a href="https://releases.hashicorp.com...">https://releases.hashicorp.com...</a>
&ndash; Downloading plugin for provider &ldquo;google&rdquo; (1.19.1)&hellip;</p>

<p>The following providers do not have any version constraints in configuration,
so the latest version was installed.</p>

<p>To prevent automatic upgrades to new major versions that may contain breaking
changes, it is recommended to add version = &ldquo;&hellip;&rdquo; constraints to the
corresponding provider blocks in configuration, with the constraint strings
suggested below.</p>

<ul>
<li>provider.google: version = &ldquo;~> 1.19&rdquo;</li>
</ul>


<p>Terraform has been successfully initialized!</p>

<p>You may now begin working with Terraform. Try running &ldquo;terraform plan&rdquo; to see
any changes that are required for your infrastructure. All Terraform commands
should now work.</p>

<p>If you ever set or change modules or backend configuration for Terraform,
rerun this command to reinitialize your working directory. If you forget, other
commands will detect it and remind you to do so if necessary.
```</p>

<p><strong>A</strong>o que parece, o nosso <em>state</em> do Terraform foi iniciado com sucesso e tudo parece correto. Vamos tentar criar nosso plano de execução agora com <em>terraform plan</em>:</p>

<p>``` yaml
$ terraform plan</p>

<p>Refreshing Terraform state in-memory prior to plan&hellip;
The refreshed state will be used to calculate this plan, but will not be
persisted to local or remote state storage.</p>

<hr />

<p>No changes. Infrastructure is up-to-date.</p>

<p>This means that Terraform did not detect any differences between your
configuration and real physical resources that exist. As a result, no
actions need to be performed.
```</p>

<p><strong>T</strong>udo parece correto e, assim como dito anteriormente, o Terraform também nos informa que tudo está atualizado e que nenhuma ação precisa ser realizada, afinal não indicamos nada a ser criado, nenhum recurso. Apenas indicamos que utilizaremos um determinado projeto em uma determinada região, e tal projeto já existe no GCP.</p>

<p><strong>V</strong>amos então iniciar a criação de nossa infraestrutura básica. Iniciemos criando uma VM simples.</p>

<p><strong>Q</strong>uando falamos em VMs na nuvem, existem alguns atributos importantes que devemos levar em conta antes de criar a mesma:</p>

<ul>
<li>Nome: Nossa VM precisa ter um nome;</li>
<li>Tipo de máquina: Quando se decide comprar um novo servidor para sua empresa, você terá diversas máquinas disponíveis a venda, algumas com mais memória, outras com menos, CPU, Disco, etc. Da mesma forma funcionam VMs em um ambiente de nuvem como o GCP. Precisamos determinar o tipo de VM que queremos;</li>
<li>Zona: Todo provedor na nuvem ou <em>cloud</em> possui diversas zonas ou regiões disponíveis espalhadas pelo globo, portanto devemos sempre informar onde queremos rodar nossos recursos;</li>
<li>Imagem: Assim como fazemos com um servidor físico, sempre devemos escolher uma imagem ou Sistema Operacional para ser instalado em nossa VM;</li>
</ul>


<p><strong>U</strong>ma vez que tenhamos iuma listagem básica de algumas variáveis importantes para nossa VM, vamos inserí-las em nosso arquivo <em>variables.tf</em>:</p>

<p>``` py variables.tf
variable &ldquo;project_id&rdquo; {
  type    = &ldquo;string&rdquo;
  default = &ldquo;possible-sun-83482736&rdquo;
}</p>

<p>variable &ldquo;regiao&rdquo; {
  type = &ldquo;string&rdquo;
  default = &ldquo;northamerica-northeast1&rdquo;
}</p>

<p>variable &ldquo;nome&rdquo; {
  type = &ldquo;string&rdquo;
  default = &ldquo;vm-webserver&rdquo;
}</p>

<p>variable &ldquo;tipo_maquina&rdquo; {
  type = &ldquo;string&rdquo;
  default = &ldquo;f1-micro&rdquo;
}</p>

<p>variable &ldquo;zona&rdquo; {
  type = &ldquo;string&rdquo;
  default = &ldquo;northamerica-northeast1-a&rdquo;
}</p>

<p>variable &ldquo;imagem&rdquo; {
  type = &ldquo;string&rdquo;
  default = &ldquo;debian-cloud/debian-9&rdquo;
}
```</p>

<p><strong>A</strong>qui criamos 4 novas variáveis. Daremos um nome (vm-webserver) para nossa VM, um tipo de máquina (f1-micro), uma zona (em meu caso utilizarei northamerica-northamerica1-a por ser a mais próxima de mim, mas sinta-se livre para utilizar a que melhor lhe servir) e uma imagem (debian-cloud/debian-9, a qual faz parte da enorme lista de imagens disponíveis no Google).</p>

<p>Agora que temos nossas variáveis definidas, vamos editar o arquivo <em>main.tf</em> para criar nossos recursos para a VM:</p>

<p>``` py main.tf</p>

<h1>Configura o projeto GCP</h1>

<p>provider &ldquo;google&rdquo; {
  credentials = &ldquo;${file("possible-sun-83482736-sabh45jhb2345ghv.json&rdquo;)}&ldquo;
  project     = &rdquo;${var.project_id}&ldquo;
  region      = &rdquo;${var.regiao}&ldquo;
}</p>

<h1>Cria a VM com o Google Compute Engine</h1>

<p>resource &ldquo;google_compute_instance&rdquo; &ldquo;webserver&rdquo; {
  name          = &ldquo;${var.nome}&rdquo;
  machine_type  = &ldquo;${var.tipo_maquina}&rdquo;
  zone          = &ldquo;${var.zona}&rdquo;</p>

<p>  boot_disk {</p>

<pre><code>initialize_params {
  image = "${var.imagem}"
}
</code></pre>

<p>  }</p>

<p>  # Instala o servidor web Apache
  metadata_startup_script = &ldquo;sudo apt-get update; sudo apt-get install apache2 -y; echo Testando > /var/www/html/index.html&rdquo;</p>

<p>  # Habilita rede para a VM bem como um IP público
  network_interface {</p>

<pre><code>network = "default"
access_config {

}
</code></pre>

<p>  }
}
```</p>

<p><strong>A</strong>qui inserimos um <em>resource</em> de tipo <em>google_compute_instance</em> para criar nossa VM e nele passamos os detalhes de nossa VM, tais como nome, tipo de máquina, zona e rede.</p>

<p><strong>E</strong>stamos também utilizando a propriedade <em>metadata_startup_script</em>, na qual o Terraform nos permite utilizar o recurso de script de inicialização do Google Cloud. Este parâmetro nos permite executar um script logo que a VM é criada, portanto podemos automatizar qualquer setup inicial de nossa VM através deste recurso. Neste exemplo estaremos apenas atualizando os repositórios de nosso Debian e instalando um servidor web Apache para nosso teste.</p>

<p><strong>A</strong>gora que temos nossos arquivos <em>variables.tf</em> e <em>main.tf</em> atualizados, vamos executar novamente <em>terraform plan</em> para ver o que aconteceria com nossa infraestrutura:</p>

<p>``` yaml
$ terraform plan
Refreshing Terraform state in-memory prior to plan&hellip;
The refreshed state will be used to calculate this plan, but will not be
persisted to local or remote state storage.</p>

<hr />

<p>An execution plan has been generated and is shown below.
Resource actions are indicated with the following symbols:
 + create</p>

<p>Terraform will perform the following actions:</p>

<ul>
<li>google_compute_instance.webserver
  id:                                                  <computed>
  boot_disk.#:                                         &ldquo;1&rdquo;
  boot_disk.0.auto_delete:                             &ldquo;true&rdquo;
  boot_disk.0.device_name:                             <computed>
  boot_disk.0.disk_encryption_key_sha256:              <computed>
  boot_disk.0.initialize_params.#:                     &ldquo;1&rdquo;
  boot_disk.0.initialize_params.0.image:               &ldquo;debian-cloud/debian-9&rdquo;
  boot_disk.0.initialize_params.0.size:                <computed>
  boot_disk.0.initialize_params.0.type:                <computed>
  can_ip_forward:                                      &ldquo;false&rdquo;
  cpu_platform:                                        <computed>
  create_timeout:                                      &ldquo;4&rdquo;
  deletion_protection:                                 &ldquo;false&rdquo;
  guest_accelerator.#:                                 <computed>
  instance_id:                                         <computed>
  label_fingerprint:                                   <computed>
  machine_type:                                        &ldquo;f1-micro&rdquo;
  metadata_fingerprint:                                <computed>
  metadata_startup_script:                             &ldquo;sudo apt-get update; sudo apt-get install apache2 -y; echo Testando > /var/www/html/index.html&rdquo;
  name:                                                &ldquo;vm-webserver&rdquo;
  network_interface.#:                                 &ldquo;1&rdquo;
  network_interface.0.access_config.#:                 &ldquo;1&rdquo;
  network_interface.0.access_config.0.assigned_nat_ip: <computed>
  network_interface.0.access_config.0.nat_ip:          <computed>
  network_interface.0.access_config.0.network_tier:    <computed>
  network_interface.0.address:                         <computed>
  network_interface.0.name:                            <computed>
  network_interface.0.network:                         &ldquo;default&rdquo;
  network_interface.0.network_ip:                      <computed>
  network_interface.0.subnetwork_project:              <computed>
  project:                                             <computed>
  scheduling.#:                                        <computed>
  self_link:                                           <computed>
  tags_fingerprint:                                    <computed>
  zone:                                                &ldquo;northamerica-northeast1-a&rdquo;</li>
</ul>


<p>Plan: 1 to add, 0 to change, 0 to destroy.</p>

<hr />

<p>Note: You didn&rsquo;t specify an &ldquo;-out&rdquo; parameter to save this plan, so Terraform
can&rsquo;t guarantee that exactly these actions will be performed if
&ldquo;terraform apply&rdquo; is subsequently run.
```</p>

<p><strong>A</strong>parentemente tudo está conforme o esperado. O Terraform criará um recurso para nossa VM.</p>

<p><strong>A</strong>ntes de aplicarmos nosso plano de fato, volte à sua <em>Dashboard</em> do GCP e, no menu principal do canto esquerdo navegue até o painel de Instâncias (VMs):</p>

<p><img class="center" src="/imgs/gcloud11.png" title="&lsquo;Google Cloud Platform&rsquo;" ></p>

<p><strong>C</strong>aso seja a sua primeira vez acessando esta seção, você deverá receber a informação de que o Google esta carregando a API de <em>Compute Engine</em> para seu uso. Este processo costuma levar cerca de 1 ou 2 minutos, mas apenas acontece na primeira vez que você acessa este painel. Para otimizar sua nuvem o Google não carrega todas as APIs por padrão, habilitando-as aos poucos conforme você as utiliza, bem como lhe permitindo habilitar apenas as que você deseja ou desabilitar as que não precisa.</p>

<p><strong>V</strong>ocê provavelmente não terá nenhuma VM ou instância criada, portanto nada será listado para você além de uma tela informando que ali ficarão suas VMs, bem como um botão de <strong>Criar</strong> VMs, através do qual você poderia criar suas VMs e passar todas as suas definições pela interface. Mas o nosso objetivo é automatizar tudo isso e codificar nossa infraestrutura, certo?! Portanto, ignoremos isto por enquanto. O nosso objetivo nesta interface era apenas ver que não temos nossa vm webserver criada (ainda).</p>

<p><strong>D</strong>e volta ao nosso console ou terminal, executemos <em>terraform apply</em>:</p>

<p>``` yaml
$ terraform apply</p>

<p>An execution plan has been generated and is shown below.
Resource actions are indicated with the following symbols:
  + create</p>

<p>Terraform will perform the following actions:</p>

<ul>
<li>google_compute_instance.webserver
  id:                                                  <computed>
  boot_disk.#:                                         &ldquo;1&rdquo;
  boot_disk.0.auto_delete:                             &ldquo;true&rdquo;
  boot_disk.0.device_name:                             <computed>
  boot_disk.0.disk_encryption_key_sha256:              <computed>
  boot_disk.0.initialize_params.#:                     &ldquo;1&rdquo;
  boot_disk.0.initialize_params.0.image:               &ldquo;debian-cloud/debian-9&rdquo;
  boot_disk.0.initialize_params.0.size:                <computed>
  boot_disk.0.initialize_params.0.type:                <computed>
  can_ip_forward:                                      &ldquo;false&rdquo;
  cpu_platform:                                        <computed>
  create_timeout:                                      &ldquo;4&rdquo;
  deletion_protection:                                 &ldquo;false&rdquo;
  guest_accelerator.#:                                 <computed>
  instance_id:                                         <computed>
  label_fingerprint:                                   <computed>
  machine_type:                                        &ldquo;f1-micro&rdquo;
  metadata_fingerprint:                                <computed>
  metadata_startup_script:                             &ldquo;sudo apt-get update; sudo apt-get install apache2 -y; echo Testando > /var/www/html/index.html&rdquo;
  name:                                                &ldquo;vm-webserver&rdquo;
  network_interface.#:                                 &ldquo;1&rdquo;
  network_interface.0.access_config.#:                 &ldquo;1&rdquo;
  network_interface.0.access_config.0.assigned_nat_ip: <computed>
  network_interface.0.access_config.0.nat_ip:          <computed>
  network_interface.0.access_config.0.network_tier:    <computed>
  network_interface.0.address:                         <computed>
  network_interface.0.name:                            <computed>
  network_interface.0.network:                         &ldquo;default&rdquo;
  network_interface.0.network_ip:                      <computed>
  network_interface.0.subnetwork_project:              <computed>
  project:                                             <computed>
  scheduling.#:                                        <computed>
  self_link:                                           <computed>
  tags_fingerprint:                                    <computed>
  zone:                                                &ldquo;northamerica-northeast1-a&rdquo;</li>
</ul>


<p>Plan: 1 to add, 0 to change, 0 to destroy.</p>

<p>Do you want to perform these actions?
  Terraform will perform the actions described above.
  Only &lsquo;yes&rsquo; will be accepted to approve.</p>

<p>  Enter a value:
```</p>

<p><strong>T</strong>udo parece correto, inclusive podemos ver nosso script de metadata que deverá ser executado durante a criação da VM para instalar nosso servidor WEB Apache. Confirme com um <em>yes</em>:</p>

<p>``` yaml
Enter a value: yes</p>

<p>google_compute_instance.webserver: Creating&hellip;
boot_disk.#:                                         &ldquo;&rdquo; => &ldquo;1&rdquo;
boot_disk.0.auto_delete:                             &ldquo;&rdquo; => &ldquo;true&rdquo;
boot_disk.0.device_name:                             &ldquo;&rdquo; => &ldquo;<computed>&rdquo;
boot_disk.0.disk_encryption_key_sha256:              &ldquo;&rdquo; => &ldquo;<computed>&rdquo;
boot_disk.0.initialize_params.#:                     &ldquo;&rdquo; => &ldquo;1&rdquo;
boot_disk.0.initialize_params.0.image:               &ldquo;&rdquo; => &ldquo;debian-cloud/debian-9&rdquo;
boot_disk.0.initialize_params.0.size:                &ldquo;&rdquo; => &ldquo;<computed>&rdquo;
boot_disk.0.initialize_params.0.type:                &ldquo;&rdquo; => &ldquo;<computed>&rdquo;
can_ip_forward:                                      &ldquo;&rdquo; => &ldquo;false&rdquo;
cpu_platform:                                        &ldquo;&rdquo; => &ldquo;<computed>&rdquo;
create_timeout:                                      &ldquo;&rdquo; => &ldquo;4&rdquo;
deletion_protection:                                 &ldquo;&rdquo; => &ldquo;false&rdquo;
guest_accelerator.#:                                 &ldquo;&rdquo; => &ldquo;<computed>&rdquo;
instance_id:                                         &ldquo;&rdquo; => &ldquo;<computed>&rdquo;
label_fingerprint:                                   &ldquo;&rdquo; => &ldquo;<computed>&rdquo;
machine_type:                                        &ldquo;&rdquo; => &ldquo;f1-micro&rdquo;
metadata_fingerprint:                                &ldquo;&rdquo; => &ldquo;<computed>&rdquo;
metadata_startup_script:                             &ldquo;&rdquo; => &ldquo;sudo apt-get update; sudo apt-get install apache2 -y; echo Testando > /var/www/html/index.html&rdquo;
name:                                                &ldquo;&rdquo; => &ldquo;vm-webserver&rdquo;
network_interface.#:                                 &ldquo;&rdquo; => &ldquo;1&rdquo;
network_interface.0.access_config.#:                 &ldquo;&rdquo; => &ldquo;1&rdquo;
network_interface.0.access_config.0.assigned_nat_ip: &ldquo;&rdquo; => &ldquo;<computed>&rdquo;
network_interface.0.access_config.0.nat_ip:          &ldquo;&rdquo; => &ldquo;<computed>&rdquo;
network_interface.0.access_config.0.network_tier:    &ldquo;&rdquo; => &ldquo;<computed>&rdquo;
network_interface.0.address:                         &ldquo;&rdquo; => &ldquo;<computed>&rdquo;
network_interface.0.name:                            &ldquo;&rdquo; => &ldquo;<computed>&rdquo;
network_interface.0.network:                         &ldquo;&rdquo; => &ldquo;default&rdquo;
network_interface.0.network_ip:                      &ldquo;&rdquo; => &ldquo;<computed>&rdquo;
network_interface.0.subnetwork_project:              &ldquo;&rdquo; => &ldquo;<computed>&rdquo;
project:                                             &ldquo;&rdquo; => &ldquo;<computed>&rdquo;
scheduling.#:                                        &ldquo;&rdquo; => &ldquo;<computed>&rdquo;
self_link:                                           &ldquo;&rdquo; => &ldquo;<computed>&rdquo;
tags_fingerprint:                                    &ldquo;&rdquo; => &ldquo;<computed>&rdquo;
zone:                                                &ldquo;&rdquo; => &ldquo;northamerica-northeast1-a&rdquo;
google_compute_instance.webserver: Still creating&hellip; (10s elapsed)
google_compute_instance.webserver: Creation complete after 14s (ID: vm-webserver)</p>

<p>Apply complete! Resources: 1 added, 0 changed, 0 destroyed.
```</p>

<p><strong>N</strong>osso código parece ter sido executado com sucesso e o Terraform ao final nos confirma que um <em>resource</em> foi adicionado (o qual esperamos ser nossa VM).</p>

<p>Se atualizarmos nossa página de instâncias no GCP, devemos agora ver nossa <em>vm-webserver</em> criada e rodando:</p>

<p><img class="center" src="/imgs/gcloud12.png" title="&lsquo;Google Cloud Platform&rsquo;" ></p>

<p><strong>A</strong>gora que temos a certeza de que nosso código funciona para a criação de nossa VM, precisamos também fazer com que a mesma seja acessível, afinal a principal função de qualquer servidor web é justamente ser acessível para apresentar alguma aplicação ou site. Para isto precisamos habilitar o firewall de nossa VM ou instância. Além disso, precisamos saber qual o endereço IP desta instância. Embora seja possível e simples conseguir este endereço IP através da interface do GCP ao clicarmos em cima de nossa VM, podemos também automatizar isto e receber este valor diretamente através do Terraform. (Lembre-se: O principal objetivo por trás da ideia de infraestrutura como código é automatizar ao máximo, evitando o uso de interfaces ou  dashboards (odeio cliques) :p).</p>

<p><strong>V</strong>amos começar criando um arquivo <em>outputs.tf</em> que será utilizado para nos informar o IP da VM criada pelo Terraform:</p>

<p>``` py outputs.tf</p>

<h1>Retorna o IP da VM criada</h1>

<p>output &ldquo;ip&rdquo; {
  value = &ldquo;${google_compute_instance.webserver.network_interface.0.access_config.0.nat_ip}&rdquo;
}
```</p>

<p><strong>N</strong>ão, eu não estou inventando comandos ou trazendo algo do além. Se você é curioso e atencioso deve ter percebido que aqui estamos apenas puxando informações que o GCP criou através do Terraform. Se você voltar e reparar em seu comando <em>terraform apply</em>, perceberá que um dos atributos listados/criados em sua VM foi justamente o <em>network_interface.0.access_config.0.nat_ip</em>:</p>

<p><code>yaml
...
network_interface.0.access_config.0.assigned_nat_ip: "" =&gt; "&lt;computed&gt;"
--&gt; network_interface.0.access_config.0.nat_ip:          "" =&gt; "&lt;computed&gt;"
network_interface.0.access_config.0.network_tier:    "" =&gt; "&lt;computed&gt;"
network_interface.0.address:                         "" =&gt; "&lt;computed&gt;"
...
</code></p>

<p><strong>P</strong>ara validarmos que isto de fato funcionará, vamos executar novamente <em>terraform apply</em>:</p>

<p>``` yaml
$ terraform apply
google_compute_instance.webserver: Refreshing state&hellip; (ID: vm-webserver)</p>

<p>Apply complete! Resources: 0 added, 0 changed, 0 destroyed.</p>

<p>Outputs:</p>

<p>ip = 35.203.117.191
```</p>

<p><strong>C</strong>onforme o esperado, nenhum <em>resource</em> precisou ser criado, pois não alteramos nada em nosso código indicando a necessidade de alguma mudança. A única coisa que aocnteceu de diferente foi que desta vez recebemos o IP de nossa instância ou VM.</p>

<p><strong>O</strong>utra forma de recebermos este valor a qualquer momento é executando <em>terraform output ip</em>:</p>

<p><code>yaml
$ terraform output ip
35.203.117.191
</code></p>

<p><strong>Ó</strong>timo, já temos nossa instância e sabemos que podemos receber o endereço IP externo dela facilmente para acesso. Só nos resta agora abrir as portas de firewall necessárias para que possamos acessar nossa aplicação. Vamos abrir a porta 80 para nossa VM.</p>

<p><strong>V</strong>amos criar as seguintes variáveis em nosso arquivo <em>variables.tf</em>:</p>

<ul>
<li>nome_fw: precisamos dar um nome para nosso firewall;</li>
<li>portas: precisamos criar uma lista de portas que serão abertas neste firewall.</li>
</ul>


<p>``` py variables.tf
variable &ldquo;project_id&rdquo; {
  type    = &ldquo;string&rdquo;
  default = &ldquo;possible-sun-83482736&rdquo;
}</p>

<p>variable &ldquo;regiao&rdquo; {
  type = &ldquo;string&rdquo;
  default = &ldquo;northamerica-northeast1&rdquo;
}</p>

<p>variable &ldquo;nome&rdquo; {
  type = &ldquo;string&rdquo;
  default = &ldquo;vm-webserver&rdquo;
}</p>

<p>variable &ldquo;tipo_maquina&rdquo; {
  type = &ldquo;string&rdquo;
  default = &ldquo;f1-micro&rdquo;
}</p>

<p>variable &ldquo;zona&rdquo; {
  type = &ldquo;string&rdquo;
  default = &ldquo;northamerica-northeast1-a&rdquo;
}</p>

<p>variable &ldquo;imagem&rdquo; {
  type = &ldquo;string&rdquo;
  default = &ldquo;debian-cloud/debian-9&rdquo;
}</p>

<p>variable &ldquo;nome_fw&rdquo; {
  type = &ldquo;string&rdquo;
  default = &ldquo;webserver-firewall&rdquo;
}</p>

<p>variable &ldquo;portas&rdquo; {
  type = &ldquo;list&rdquo;
  default = [&ldquo;80&rdquo;]
}
```</p>

<p><strong>A</strong> única novidade aqui foi o tipo <em>list</em> que utilizamos para a variável <em>portas</em>. Uma vez que podemos querer abrir múltiplas portas, utilizaremos o tipo <em>list</em>, o qual nos permite ter vários valores, ou uma lista de valores, para esta variável.</p>

<p><strong>A</strong>gora vamos criar o <em>resource</em> para nosso firewall no arquivo <em>main.tf</em>:</p>

<p>``` py main.tf</p>

<h1>Configura o projeto GCP</h1>

<p>provider &ldquo;google&rdquo; {
  credentials = &ldquo;${file("possible-sun-83482736-sabh45jhb2345ghv.json&rdquo;)}&ldquo;
  project     = &rdquo;${var.project_id}&ldquo;
  region      = &rdquo;${var.regiao}&ldquo;
}</p>

<h1>Cria a VM com o Google Compute Engine</h1>

<p>resource &ldquo;google_compute_instance&rdquo; &ldquo;webserver&rdquo; {
  name          = &ldquo;${var.nome}&rdquo;
  machine_type  = &ldquo;${var.tipo_maquina}&rdquo;
  zone          = &ldquo;${var.zona}&rdquo;</p>

<p>  boot_disk {</p>

<pre><code>initialize_params {
  image = "${var.imagem}"
}
</code></pre>

<p>  }</p>

<p>  # Instala o servidor web Apache
  metadata_startup_script = &ldquo;sudo apt-get update; sudo apt-get install apache2 -y; echo Testando > /var/www/html/index.html&rdquo;</p>

<p>  # Habilita rede para a VM bem como um IP público
  network_interface {</p>

<pre><code>network = "default"
access_config {

}
</code></pre>

<p>  }
}</p>

<h1>Cria o Firewall para a VM</h1>

<p>resource &ldquo;google_compute_firewall&rdquo; &ldquo;webfirewall&rdquo; {
  name        = &ldquo;${var.nome_fw}&rdquo;
  network     = &ldquo;default&rdquo;</p>

<p>  allow {</p>

<pre><code>protocol  = "tcp"
ports     = "${var.portas}"
</code></pre>

<p>  }
}</p>

<p>```</p>

<p><strong>I</strong>ncluímos apenas um novo <em>resource</em> para a criação de nosso firewall abrindo as portas que definimos em nosso arquivo <em>variables.tf</em> com o protocolo <em>tcp</em>. Execute novamente <em>terraform plan</em> para ver o que mudaria:</p>

<p>``` yaml
$ terraform plan
Refreshing Terraform state in-memory prior to plan&hellip;
The refreshed state will be used to calculate this plan, but will not be
persisted to local or remote state storage.</p>

<p>google_compute_instance.webserver: Refreshing state&hellip; (ID: vm-webserver)</p>

<hr />

<p>An execution plan has been generated and is shown below.
Resource actions are indicated with the following symbols:
  + create</p>

<p>Terraform will perform the following actions:</p>

<ul>
<li>google_compute_firewall.webfirewall
  id:                       <computed>
  allow.#:                  &ldquo;1&rdquo;
  allow.272637744.ports.#:  &ldquo;1&rdquo;
  allow.272637744.ports.0:  &ldquo;80&rdquo;
  allow.272637744.protocol: &ldquo;tcp&rdquo;
  creation_timestamp:       <computed>
  destination_ranges.#:     <computed>
  direction:                <computed>
  name:                     &ldquo;webserver-firewall&rdquo;
  network:                  &ldquo;default&rdquo;
  priority:                 &ldquo;1000&rdquo;
  project:                  <computed>
  self_link:                <computed>
  source_ranges.#:          <computed></li>
</ul>


<p>Plan: 1 to add, 0 to change, 0 to destroy.</p>

<hr />

<p>Note: You didn&rsquo;t specify an &ldquo;-out&rdquo; parameter to save this plan, so Terraform
can&rsquo;t guarantee that exactly these actions will be performed if
&ldquo;terraform apply&rdquo; is subsequently run.
```</p>

<p><strong>T</strong>udo parece correto. Como a VM já existe e nada foi modificado no código da mesma, o Terraform apenas criará um  novo <em>resource</em> para nosso firewall com os valores que definimos. Execute seu plano com <em>terraform apply</em> e confirme com <em>yes</em>:</p>

<p>``` yaml
$ terraform apply
google_compute_instance.webserver: Refreshing state&hellip; (ID: vm-webserver)</p>

<p>An execution plan has been generated and is shown below.
Resource actions are indicated with the following symbols:
  + create</p>

<p>Terraform will perform the following actions:</p>

<ul>
<li>google_compute_firewall.webfirewall
  id:                       <computed>
  allow.#:                  &ldquo;1&rdquo;
  allow.272637744.ports.#:  &ldquo;1&rdquo;
  allow.272637744.ports.0:  &ldquo;80&rdquo;
  allow.272637744.protocol: &ldquo;tcp&rdquo;
  creation_timestamp:       <computed>
  destination_ranges.#:     <computed>
  direction:                <computed>
  name:                     &ldquo;webserver-firewall&rdquo;
  network:                  &ldquo;default&rdquo;
  priority:                 &ldquo;1000&rdquo;
  project:                  <computed>
  self_link:                <computed>
  source_ranges.#:          <computed></li>
</ul>


<p>Plan: 1 to add, 0 to change, 0 to destroy.</p>

<p>Do you want to perform these actions?
  Terraform will perform the actions described above.
  Only &lsquo;yes&rsquo; will be accepted to approve.</p>

<p>  Enter a value: yes</p>

<p>google_compute_firewall.webfirewall: Creating&hellip;
  allow.#:                  &ldquo;&rdquo; => &ldquo;1&rdquo;
  allow.272637744.ports.#:  &ldquo;&rdquo; => &ldquo;1&rdquo;
  allow.272637744.ports.0:  &ldquo;&rdquo; => &ldquo;80&rdquo;
  allow.272637744.protocol: &ldquo;&rdquo; => &ldquo;tcp&rdquo;
  creation_timestamp:       &ldquo;&rdquo; => &ldquo;<computed>&rdquo;
  destination_ranges.#:     &ldquo;&rdquo; => &ldquo;<computed>&rdquo;
  direction:                &ldquo;&rdquo; => &ldquo;<computed>&rdquo;
  name:                     &ldquo;&rdquo; => &ldquo;webserver-firewall&rdquo;
  network:                  &ldquo;&rdquo; => &ldquo;default&rdquo;
  priority:                 &ldquo;&rdquo; => &ldquo;1000&rdquo;
  project:                  &ldquo;&rdquo; => &ldquo;<computed>&rdquo;
  self_link:                &ldquo;&rdquo; => &ldquo;<computed>&rdquo;
  source_ranges.#:          &ldquo;&rdquo; => &ldquo;<computed>&rdquo;
google_compute_firewall.webfirewall: Still creating&hellip; (10s elapsed)
google_compute_firewall.webfirewall: Creation complete after 14s (ID: webserver-firewall)</p>

<p>Apply complete! Resources: 1 added, 0 changed, 0 destroyed.</p>

<p>Outputs:</p>

<p>ip = 35.203.117.191
```</p>

<p><strong>T</strong>udo parece ter funcionado. Um novo resource de <em>firewall</em> foi criado e, conforme visto anteriormente, também recebemos o nosso IP como output.</p>

<p><strong>C</strong>onfirme que tudo funcionou como o esperado copiando este IP e colando-o em seu navegador:</p>

<p><img class="center" src="/imgs/gcloud13.png" title="&lsquo;Google Cloud Platform&rsquo;" ></p>

<p><strong>S</strong>e você recebeu uma página em branco com a palavra <em>Testando</em>, significa que tudo ocorreu conforme o esperado.</p>

<p><strong>A</strong>té agora nossa infraestrutura possui:</p>

<ul>
<li>1 VM como servidor Web básico rodando Apache e uma página de teste;</li>
<li>1 rede default;</li>
<li>1 firewall básico aplicado à nossa VM abrindo a porta 80.</li>
</ul>


<p><strong>E</strong>mbora esta seja uma infraestrutura extremamente simples, o Terraform lhe permite criar infraestruturas muito mais complexas e robustas.</p>

<p><strong>N</strong>o próximo post incrementaremos este código para incluírmos a criação de um cluster Kubernetes em nossa <em>cloud</em>.</p>

<p><strong>P</strong>or hora, para não consumir muito de nossos créditos no GCP, vamos destruir nossa infraestrutura com <em>terraform destroy</em>. É tão simples e rápido criar tudo novamente agora que temos o código pronto, certo?! Então destruir tudo não nos causará problemas.</p>

<p>``` yaml
$ terraform destroy
google_compute_firewall.webfirewall: Refreshing state&hellip; (ID: webserver-firewall)
google_compute_instance.webserver: Refreshing state&hellip; (ID: vm-webserver)</p>

<p>An execution plan has been generated and is shown below.
Resource actions are indicated with the following symbols:
  &ndash; destroy</p>

<p>Terraform will perform the following actions:</p>

<ul>
<li><p>google_compute_firewall.webfirewall</p></li>
<li><p>google_compute_instance.webserver</p></li>
</ul>


<p>Plan: 0 to add, 0 to change, 2 to destroy.</p>

<p>Do you really want to destroy all resources?
  Terraform will destroy all your managed infrastructure, as shown above.
  There is no undo. Only &lsquo;yes&rsquo; will be accepted to confirm.</p>

<p>  Enter a value: yes</p>

<p>google_compute_firewall.webfirewall: Destroying&hellip; (ID: webserver-firewall)
google_compute_instance.webserver: Destroying&hellip; (ID: vm-webserver)
google_compute_firewall.webfirewall: Still destroying&hellip; (ID: webserver-firewall, 10s elapsed)
google_compute_instance.webserver: Still destroying&hellip; (ID: vm-webserver, 10s elapsed)
google_compute_firewall.webfirewall: Destruction complete after 12s
google_compute_instance.webserver: Destruction complete after 14s</p>

<p>Destroy complete! Resources: 2 destroyed.
```</p>

<p><strong>T</strong>udo certo, sua infra foi completamente removida e seus créditos não mais serão utilizados agora.</p>

<p><strong>H</strong>appy hacking!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Conhecendo o Kubernetes - Clusters de Containers]]></title>
    <link href="https://kalib.github.io/blog/2017/06/24/conhecendo-o-kubernetes-clusters-de-containers/"/>
    <updated>2017-06-24T09:32:00-04:00</updated>
    <id>https://kalib.github.io/blog/2017/06/24/conhecendo-o-kubernetes-clusters-de-containers</id>
    <content type="html"><![CDATA[<p><img class="center" src="/imgs/kubernetes_logo.png" title="&lsquo;Kubernetes&rsquo;" ></p>

<h2>O que é Kubernetes</h2>

<p><strong>K</strong>ubernetes é uma solução Open Source desenvolvida pelo Google, originalmente chamada de K8s, como uma ferramenta para gerenciar clusters de containers (ou containeres, como prefira). Em 2005, quando a ferramenta foi desenvolvida, originalmente para uso interno, o Google doou o código à recém fundada <a href="https://www.cncf.io/">Cloud Native Computing Foundation</a>, em parceria com a <a href="https://www.linuxfoundation.org/">The Linux Foundation</a>.</p>

<p><strong>O</strong> motivo do leme em sua logomarca é devido à origem grega da palavra, que vem de Kuvernetes, que representa a pessoa que pilota o navio, timoneiro.</p>

<p><strong>C</strong>omo objetivo primário o Kubernetes provê uma plataforma de automação para deployments, escalonamento e operações de containers de aplicações em um cluster de hosts ou nodes.</p>

<p><strong>A</strong>ntes de seguir com a explicação, instalação e configuração do Kubernetes, estou supondo que você já possui algum conhecimento básico sobre o que sejam containers e tenha alguma familiaridade com o <a href="https://www.docker.com">Docker</a>. Caso não possua um entendimento básico sobre containers e Docker, sugiro que leia algo antes de seguir com este artigo. Possuo um post introdutório sobre containers com um exemplo básico e prático sobre como criar containers com Docker, bem como iniciar uma simples aplicação web &ndash;  <a href="/blog/2015/08/20/docker-uma-alternativa-elegante-para-containers-no-linux/">aqui</a>.</p>

<p><strong>O</strong> Kubernetes é formado por uma série de componentes ou blocos que, quando utilizados coletivamente, fornecem um método de deployment, manutenção e escalonamento de clusters de aplicações baseadas em containers. Estes componentes, ou primitives como o Kubernetes os chama, foram desenvolvidos com o intuito de serem independentes, de forma que quase não se faz necessário ter conhecimento entre si para que possam funcionar e trabalhar juntos, visto que todos se comunicam e interligam através de uma API, sejam componentes internos do Kubernetes ou mesmo extensões e containers.</p>

<p><strong>E</strong>mbora tenha sido inicialmente desenvolvido para o deployment e utilização de <strong>bilhões de containers</strong> internamente no Google, desde que seu código passou a ser distribuído abertamente com a licença <a href="https://commons.apache.org/proper/commons-daemon/license.html">Apache Commons</a> o Kubernetes tem sido adotado formalmente por praticamente todos os grandes provedores de serviços em nuvem.</p>

<h2>Arquitetura do Kubernetes</h2>

<p><img class="center" src="/imgs/kubernetes_architecture.png" title="&lsquo;Kubernetes Architecture&rsquo;" ></p>

<p><strong>D</strong>entre os principais componentes do Kubernetes, vamos destacar os seguintes:</p>

<ul>
<li><p><strong>Master ou Master Controller</strong> &ndash; Host que será o gerenciador principal do Kubernetes, responsável por gerenciar os Minions ou Nodes do Cluster;</p></li>
<li><p><strong>Nodes ou Minions</strong> &ndash; Embora normalmente a nomenclatura em diversos serviços de tecnolocia seja Node, o Kubernetes prefere chamar de Minions os hosts que fazem parte de um Cluster gerenciado pelo próprio Kubernetes. Este minion pode ser um servidor físico ou virtual, necessitando possuir um serviço de gerenciamento de containers, como o Docker, por exemplo;</p></li>
<li><p><strong>ETCD</strong> &ndash; Embora este seja um serviço independente, estou listando-o aqui pois este será fundamental em seu ciclo de desenvolvimento com o Kubernetes. Cada Minion deverá rodar o <a href="https://coreos.com/etcd/docs/latest/">ETCD</a> (serviço de comunicação e gerenciamento de configurações no formato par de Chave/Valor). O ETCD é utilizado para troca e armazenamento de informações sobre os containers, pods, minions, etc.</p></li>
<li><p><strong>Pods</strong> &ndash; São grupos de containers (um ou mais) rodando em um único minion do cluster. Cada Pod receberá um endereço IP único no Cluster como forma de possibilitar a utilização de portas sem a necessidade de se preocupar com conflitos;</p></li>
<li><p><strong>Labels</strong> &ndash; São informações de identificação na configuração e gerenciamento dos objetos (como Pods ou Minions) formados de pares &ldquo;chave:valor&rdquo;;</p></li>
<li><p><strong>Controllers</strong> &ndash; Além do Master Controller, dependendo do tamanho de sua infraestrutura e quantidade de Pods e Minions, você pode optar por ter mais de um Controller para dividir a carga e tarefas de gerenciamento. Os Controllers gerenciam um grupo de pods e, dependendo do estado de configuração desejada, podem acionar outros Controllers para lidar com as replicações e escalonamento. Os Controllers também são responsáveis pela substituiçao de Pods, caso um entre em estado de falha.</p></li>
</ul>


<h2>Instalação</h2>

<p><strong>V</strong>amos ao que interessa&hellip;</p>

<p><strong>N</strong>ovamente estou supondo que você já possui alguma familiaridade com Containers, Docker e, por consequência, com GNU/Linux.</p>

<p><strong>E</strong>estarei utilizando 4 servidores virtuais rodando CentOS 7 nos exemplos a seguir, mas fica a seu critério decidir quantos utilizar.</p>

<p><em>Certamente você optar por utilizar outra distribuição, seja Debian, Ubuntu, etc.. Uma vez que optei pelo CentOS 7, estarei utilizando comandos voltados para esta distro, mas sinta-se livre para adaptar seus comandos, como substituir o &ldquo;yum&rdquo; pelo &ldquo;apt-get&rdquo;, &ldquo;pacman&rdquo;, etc..</em></p>

<p><strong>E</strong>m minha configuração chamarei os servidores da seguinte forma:</p>

<ul>
<li>centos-master</li>
<li>centos-minion1</li>
<li>centos-minion2</li>
<li>centos-minion3</li>
</ul>


<p><strong>A</strong> primeira coisa que se deve fazer sempre que se pensa em trabalhar com clusters, independente de ser um cluster de containers ou não, é ter a certeza de que os servidores terão uma correta sincronização de relógios entre si. A forma mais simples e eficiente no nosso contexto é com a utilização do NTP, portanto comece instalando o NTP nos 4 servidores, bem como habilitando o serviço e iniciando-o:</p>

<p>```</p>

<h1>yum install -y ntp</h1>

<p>```</p>

<p>```</p>

<h1>systemctl enable ntpd &amp;&amp; systemctl start ntpd</h1>

<p>```</p>

<p><strong>C</strong>aso queira certificar-se de que o serviço está realmente rodando:</p>

<p>```</p>

<h1>systemctl status ntpd</h1>

<p>● ntpd.service &ndash; Network Time Service
   Loaded: loaded (/usr/lib/systemd/system/ntpd.service; enabled; vendor preset: disabled)
   Active: active (running) since Sat 2017-06-24 17:46:02 UTC; 3s ago
  Process: 1586 ExecStart=/usr/sbin/ntpd -u ntp:ntp $OPTIONS (code=exited, status=0/SUCCESS)
 Main PID: 1587 (ntpd)
   Memory: 2.1M
   CGroup: /system.slice/ntpd.service</p>

<pre><code>       └─1587 /usr/sbin/ntpd -u ntp:ntp -g
</code></pre>

<p>```</p>

<p><strong>P</strong>inga?</p>

<p><strong>É</strong> importante nos certificarmos de que os servidores conseguem se comunicar e de que conseguem resolver nomes corretamente.</p>

<p><strong>N</strong>este exemplo, conforme informado mais acima, estamos utilizando 4 servidores com os seguintes nomes: <em>centos-master</em>, <em>centos-minion1</em>, <em>centos-minion2</em> e <em>centos-minion3</em>, portanto vamos editar o arquivo <strong>/etc/hosts</strong> de cada um deles para que possam se comunicar pelos nomes que desejamos:</p>

<p><strong>Insira as seguintes linhas no arquivo /etc/hosts dos 4 servidores:</strong></p>

<p><em>Lembre-se de substituir os IPs pelos IPs dos servidores em seu ambiente</em></p>

<p>```</p>

<h1>Ip local do servidor master</h1>

<p>172.31.22.126   centos-master</p>

<h1>Ip local do minion1</h1>

<p>172.31.120.16   centos-minion1</p>

<h1>Ip local do minion2</h1>

<p>172.31.25.6     centos-minion2</p>

<h1>Ip local do minion3</h1>

<p>172.31.123.22   centos-minion3
```</p>

<p>Feito isto, tente pingar do master para os 3 minions utilizando os nomes especificados no /etc/hosts:</p>

<p>```
[root@kalib1 ~]# ping centos-minion1
PING centos-minion1 (172.31.120.16) 56(84) bytes of data.
64 bytes from centos-minion1 (172.31.120.16): icmp_seq=1 ttl=64 time=1.06 ms</p>

<p>[root@kalib1 ~]# ping centos-minion2
PING centos-minion2 (172.31.25.6) 56(84) bytes of data.
64 bytes from centos-minion2 (172.31.25.6): icmp_seq=1 ttl=64 time=0.588 ms</p>

<p>[root@kalib1 ~]# ping centos-minion3
PING centos-minion3 (172.31.123.22) 56(84) bytes of data.
64 bytes from centos-minion3 (172.31.123.22): icmp_seq=1 ttl=64 time=1.24 ms
```</p>

<p><strong>V</strong>ocê pode realizar o mesmo teste a partir dos minions, pingando entre si e também para o centos-master.</p>

<p><strong>U</strong>ma vez que tenhamos certeza de que todos os hosts se comunicam, é hora de instalar mais alguns pacotes necessários.</p>

<p><strong>P</strong>rimeiramente, vamos configurar o repositório do Docker para o CentOS 7:</p>

<ul>
<li>Configurações de repositório retiradas dos repositórios CBS do Centos: <a href="https://cbs.centos.org/repos/virt7-docker-common-release/">https://cbs.centos.org/repos/virt7-docker-common-release/</a></li>
</ul>


<p><strong>V</strong>amos criar o seguinte arquivo de repositório:</p>

<p>```</p>

<h1>vim /etc/yum.repos.d/virt7-docker-common-release.repos</h1>

<p>```</p>

<p><strong>O</strong> conteúdo deste arquivo será o seguinte:</p>

<p><code>
[virt7-docker-common-release]
name=virt7-docker-common-release
baseurl=https://cbs.centos.org/repos/virt7-docker-common-release/x86_64/os/
gpgcheck=0
</code></p>

<p><em>Este arquivo deverá ser criado nos 4 servidores.</em></p>

<p><strong>E</strong>m seguida, vamos atualizar a nossa base de repositórios e pacotes, também <strong>nos 4 servidores</strong>, bem como habilitar o novo repositório para instalar os pacotes docker e kubernetes:</p>

<p>```</p>

<h1>yum update</h1>

<p>```</p>

<p>```</p>

<h1>yum install -y &mdash;enablerepo=virt7-docker-common-release docker kubernetes</h1>

<p>```</p>

<p><strong>C</strong>omo dito na introdução, também precisaremos do etcd para o armazenamento e troca de configurações, portanto vamos instalá-lo também nos 4 hosts:</p>

<p>```</p>

<h1>yum instal -y etcd</h1>

<p>```</p>

<h2>Configuração</h2>

<p><strong>V</strong>amos começar com a configuração básica dos serviços envolvidos. Primeiramente, vamos abrir o arquivo de configuração do kubernetes e fazer algumas alterações:</p>

<p><em>/etc/kubernetes/config</em></p>

<p><strong>N</strong>o arquivo config altere as seguintes linhas:</p>

<p><em>Edite o valor do parâmetro KUBE_MASTER, de forma que nosso master possa ser encontrado pelo nome que definimos no hosts file. O valor original é &ldquo;&mdash;master=<a href="https://127.0.0.1:8080">https://127.0.0.1:8080</a>&rdquo;, portanto mudaremos para o seguinte:</em></p>

<p><code>
KUBE_MASTER="--master=https://centos-master:8080"
</code></p>

<p><strong>A</strong>inda neste arquivo de configuração, vamos inserir a configuração do serviço ETCD, portanto inclua a seguinte linha ao final do arquivo:</p>

<p><code>
KUBE_ETCD_SERVERS="--etcd-servers=https://centos-master:2379"
</code></p>

<p><strong>S</strong>eu arquivo de configuração deverá estar similar a este:</p>

<p>```</p>

<h2>#</h2>

<h1>kubernetes system config</h1>

<p>#</p>

<h1>The following values are used to configure various aspects of all</h1>

<h1>kubernetes services, including</h1>

<p>#</p>

<h1>kube-apiserver.service</h1>

<h1>kube-controller-manager.service</h1>

<h1>kube-scheduler.service</h1>

<h1>kubelet.service</h1>

<h1>kube-proxy.service</h1>

<h1>logging to stderr means we get it in the systemd journal</h1>

<p>KUBE_LOGTOSTDERR=&ldquo;&mdash;logtostderr=true&rdquo;</p>

<h1>journal message level, 0 is debug</h1>

<p>KUBE_LOG_LEVEL=&ldquo;&mdash;v=0&rdquo;</p>

<h1>Should this cluster be allowed to run privileged docker containers</h1>

<p>KUBE_ALLOW_PRIV=&ldquo;&mdash;allow-privileged=false&rdquo;</p>

<h1>How the controller-manager, scheduler, and proxy find the apiserver</h1>

<p>KUBE_MASTER=&ldquo;&mdash;master=<a href="https://centos-master:8080">https://centos-master:8080</a>&rdquo;</p>

<p>KUBE_ETCD_SERVERS=&ldquo;&mdash;etcd-servers=<a href="https://centos-master:2379">https://centos-master:2379</a>&rdquo;
```</p>

<p><strong>R</strong>epita esta mesma configuração nos 4 hosts. Todos eles devem utilizar exatamente os mesmos valores utilizados aqui, apontando KUBE_MASTER e KUBE_ETCD_SERVERS para centos-master, visto que este será o responsável por gerenciar todos os nossos minions.</p>

<p><strong>U</strong>ma vez que o arquivo de configuração do kubernetes esteja pronto nos 4 hosts, vamos configurar o serviço de API do kubernetes:</p>

<p><em>/etc/kubernetes/apiserver</em></p>

<p><strong>Esta configuração abaixo será apenas para o Master.</strong></p>

<p><em>Edite o valor do parâmetro KUBE_API_ADDRESS, que originalmente é &ldquo;&mdash;insecure-bind-address=127.0.0.1&rdquo;, de forma que possamos novamente receber comunicação dos demais hosts.</em></p>

<p><code>
KUBE_API_ADDRESS="--address=0.0.0.0"
</code></p>

<p><em>Descomente as linhas KUBE_API_PORT e KUBELET_PORT, para que possamos estabelecer as portas de comunicação com a API:</em></p>

<p>```</p>

<h1>The port on the local server to listen on.</h1>

<p>KUBE_API_PORT=&ldquo;&mdash;port=8080&rdquo;</p>

<h1>Port minions listen on</h1>

<p>KUBELET_PORT=&ldquo;&mdash;kubelet-port=10250&rdquo;
```</p>

<p><em>Em nosso exemplo não utilizaremos o parâmetro KUBE_ADMISSION_CONTROL, o qual nos permite ter mais controles e restrições sobre quais nodes ou minios podem entrar em nosso ambiente, portanto vamos apenas comentar esta linha por enquanto:</em></p>

<p>```</p>

<h1>default admission control policies</h1>

<h1>KUBE_ADMISSION_CONTROL=&ldquo;&mdash;admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota&rdquo;</h1>

<p>```</p>

<p><em>Nosso arquivo /etc/kubernetes/apiserver deverá estar assim:</em></p>

<p>```</p>

<h1>#</h1>

<h1>kubernetes system config</h1>

<p>#</p>

<h1>The following values are used to configure the kube-apiserver</h1>

<p>#</p>

<h1>The address on the local server to listen to.</h1>

<p>KUBE_API_ADDRESS=&ldquo;&mdash;address=0.0.0.0&rdquo;</p>

<h1>The port on the local server to listen on.</h1>

<p>KUBE_API_PORT=&ldquo;&mdash;port=8080&rdquo;</p>

<h1>Port minions listen on</h1>

<p>KUBELET_PORT=&ldquo;&mdash;kubelet-port=10250&rdquo;</p>

<h1>Comma separated list of nodes in the etcd cluster</h1>

<p>KUBE_ETCD_SERVERS=&ldquo;&mdash;etcd-servers=<a href="https://127.0.0.1:2379">https://127.0.0.1:2379</a>&rdquo;</p>

<h1>Address range to use for services</h1>

<p>KUBE_SERVICE_ADDRESSES=&ldquo;&mdash;service-cluster-ip-range=10.254.0.0/16&rdquo;</p>

<h1>default admission control policies</h1>

<h1>KUBE_ADMISSION_CONTROL=&ldquo;&mdash;admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota&rdquo;</h1>

<h1>Add your own!</h1>

<p>KUBE_API_ARGS=&ldquo;&rdquo;
```</p>

<p><strong>S</strong>alve e feche o arquivo. Novamente, esta configuração deve ser feita apenas para o Master.</p>

<p><strong>A</strong>gora vamos configurar o serviço ETCD:</p>

<p><em>/etc/etcd/etcd.conf</em></p>

<p><strong>Esta configuração abaixo será apenas para o Master.</strong></p>

<p><em>Edite os valores dos parâmetros ETCD_LISTEN_CLIENT_URLS e ETCD_ADVERTISE_CLIENT_URLS, que originalmente apontam para localhost. Como desejamos que nosso etcd escute requisições dos demais hosts, altere para o seguinte:</em></p>

<p><code>
ETCD_LISTEN_CLIENT_URLS="https://0.0.0.0:2379"
...
...
ETCD_ADVERTISE_CLIENT_URLS="https://0.0.0.0:2379"
</code></p>

<p><strong>N</strong>ovamente, não é necessário alterar a configuração do etcd nos demais hosts, apenas no Master.</p>

<p><strong>U</strong>ma vez que as configurações iniciais foram feitas, vamos habilitar e iniciar os serviços necessários <strong>no Master</strong>, sendo eles:</p>

<ul>
<li>etcd</li>
<li>kube-apiserver</li>
<li>kube-controller-manager</li>
<li>kube-scheduler</li>
</ul>


<p>```</p>

<h1>systemctl enable etcd kube-apiserver kube-controller-manager kube-scheduler</h1>

<p>```</p>

<p>```</p>

<h1>systemctl start etcd kube-apiserver kube-controller-manager kube-scheduler</h1>

<p>```</p>

<p><strong>O</strong>s 4 serviços devem estar rodando. Para termos certeza, vamos checar o status dos mesmos:</p>

<p>```</p>

<h1>systemctl status etcd kube-apiserver kube-controller-manager kube-scheduler | grep &ldquo;(running)&rdquo;</h1>

<p>   Active: active (running) since Sat 2017-06-24 21:45:37 UTC; 1min ago
   Active: active (running) since Sat 2017-06-24 21:46:13 UTC; 1min ago
   Active: active (running) since Sat 2017-06-24 21:44:25 UTC; 1min ago
   Active: active (running) since Sat 2017-06-24 21:44:25 UTC; 1min ago
```</p>

<p><strong>N</strong>ovamente, estes serviços serão iniciados no Master, e não nos nodes/minions, visto que estes utilizarão outros serviços.</p>

<p><strong>A</strong>gora vamos configurar o seguinte arquivo nos nodes/minions:</p>

<p><em>/etc/kubernetes/kubelet</em></p>

<p><strong>Este arquivo apenas deverá ser editado nos nodes/minions, não no Master.</strong></p>

<p><em>Vamos alterar o valor do parâmetro KUBELET_ADDRESS para que aceite comunicação não apenas do localhost:</em></p>

<p><code>
KUBELET_ADDRESS="--address=0.0.0.0"
</code></p>

<p><em>Descomentaremos também a linha KUBELET_PORT, para que possamos ter uma porta definida para a comunicação do kubelet:</em></p>

<p>```</p>

<h1>The port for the info server to serve on</h1>

<p>KUBELET_PORT=&ldquo;&mdash;port=10250&rdquo;
```</p>

<p><em>Vamos alterar o valor do parâmetro KUBELET_HOSTNAME para o nome que definimos <strong>para cada um dos minions</strong>, portanto em cada um deles este será um valor diferente. Supondo que este seja o minion1, utilizaremos:</em></p>

<p><code>
KUBELET_HOSTNAME="--hostname-override=centos-minion1"
</code></p>

<p><em>Vamos também alterar o valor para KUBELET_API_SERVER, apontando para o nosso Master:</em></p>

<p><code>
KUBELET_API_SERVER="--api-servers=https://centos-master:8080"
</code></p>

<p><em>Vamos comentar a linha KUBELET_POD_INFRA_CONTAINER, visto que não utilizaremos uma infraestrutura de containers externa, pois estaremos utilizando nossos próprios PODs e containers:</em></p>

<p>```</p>

<h1>pod infrastructure container</h1>

<h1>KUBELET_POD_INFRA_CONTAINER=&ldquo;&mdash;pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest&rdquo;</h1>

<p>```</p>

<p><em>Nosso arquivo deverá estar assim: (Lembrando que o parâmetro KUBELET_HOSTNAME deverá ser diferente para cada um dos 3 minions, respectivamente: centos-minion1, centos-minion2 e centos-minion3)</em></p>

<p>```</p>

<h2>#</h2>

<h1>kubernetes kubelet (minion) config</h1>

<h1>The address for the info server to serve on (set to 0.0.0.0 or &ldquo;&rdquo; for all interfaces)</h1>

<p>KUBELET_ADDRESS=&ldquo;&mdash;address=0.0.0.0&rdquo;</p>

<h1>The port for the info server to serve on</h1>

<p>KUBELET_PORT=&ldquo;&mdash;port=10250&rdquo;</p>

<h1>You may leave this blank to use the actual hostname</h1>

<p>KUBELET_HOSTNAME=&ldquo;&mdash;hostname-override=centos-minion1&rdquo;</p>

<h1>location of the api-server</h1>

<p>KUBELET_API_SERVER=&ldquo;&mdash;api-servers=<a href="https://centos-master:8080">https://centos-master:8080</a>&rdquo;</p>

<h1>pod infrastructure container</h1>

<h1>KUBELET_POD_INFRA_CONTAINER=&ldquo;&mdash;pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest&rdquo;</h1>

<h1>Add your own!</h1>

<p>KUBELET_ARGS=&ldquo;&rdquo;
```</p>

<p><strong>U</strong>ma vez que estas configurações também estão feitas nos 3 minions, vamos habilitar e iniciar os serviços necessários nos minions:</p>

<ul>
<li>kube-proxy</li>
<li>kube-kubelet</li>
<li>docker</li>
</ul>


<p>```</p>

<h1>systemctl enable kube-proxy kubelet docker</h1>

<p>```</p>

<p>```</p>

<h1>systemctl start kube-proxy kubelet docker</h1>

<p>```</p>

<p><strong>N</strong>ovamente, vamos ter certeza de que os 3 serviços estão rodando:</p>

<p>```</p>

<h1>systemctl status kube-proxy kubelet docker | grep &ldquo;(running)&rdquo;</h1>

<p>   Active: active (running) since Sat 2017-06-24 21:44:23 UTC; 1h 16min ago
   Active: active (running) since Sat 2017-06-24 21:44:27 UTC; 1h 16min ago
   Active: active (running) since Sat 2017-06-24 21:44:27 UTC; 1h 16min ago
```</p>

<p><strong>N</strong>ovamente, estes 3 serviços devem ser habilitados e iniciados nos 3 minions.</p>

<p><strong>N</strong>este momento já temos nosso cluster rodando, com um master e 3 minions. :D</p>

<h2>Testando o Cluster com o Kubernetes</h2>

<p><strong>A</strong>gora que temos a configuração básica de nosso Master Controller e de 3 minions, vamos testar nosso cluster.</p>

<p><strong>U</strong>tilizaremos o utilitário kubectl (KubeControl) disponível com o kubernetes. Caso tenha interesse em ver os parâmetros e funções do mesmo&hellip; <em>$ man kubectl</em></p>

<p><strong>V</strong>amos verificar a lista dos nodes ou minions que temos neste momento registrados em nosso Cluster. Vamos digitar alguns comandos em nosso Master Controller (centos-master):</p>

<p><code>
[root@kalib1 ~]# kubectl get nodes
NAME             STATUS    AGE
centos-minion1   Ready     17m
centos-minion2   Ready     15m
centos-minion3   Ready     10m
</code></p>

<p><strong>O</strong>s três nodes criados e configurados anteriormente já são reconhecidos pelo nosso Kubernetes através do Master Controller. Além de registrados, estão com o status Ready, o que indica que estão prontos para funcionar e executar o que precisarmos.</p>

<p><em>Caso deseje conhecer mais parâmetros que a função <strong>get</strong> do kubectl possui, podemos invocar o manual desta função: $ man kubectl-get</em></p>

<p><strong>A</strong>lém do status, podemos conseguir diversas outras informações dos nodes através do <em>kubectl</em>: <em>(Ex: kubectl describe nodes)</em> Isto lhe daria informações sobre todos os nodes. Vamos experimentar com um node em específico.</p>

<p>```
[root@kalib1 ~]# kubectl describe node centos-minion1
Name:                   centos-minion1
Role:
Labels:                 beta.kubernetes.io/arch=amd64</p>

<pre><code>                    beta.kubernetes.io/os=linux
                    kubernetes.io/hostname=centos-minion1
</code></pre>

<p>Taints:                 <none>
CreationTimestamp:      Tue, 20 Jun 2017 19:27:31 +0000
Phase:
Conditions:
  Type                  Status  LastHeartbeatTime                       LastTransitionTime                      Reason     Message</p>

<hr />

<p>  OutOfDisk             False   Sun, 25 Jun 2017 01:39:38 +0000         Fri, 23 Jun 2017 17:31:44 +0000         KubeletHasSufficientDisk    kubelet has sufficient disk space available
  MemoryPressure        False   Sun, 25 Jun 2017 01:39:38 +0000         Tue, 20 Jun 2017 19:27:31 +0000         KubeletHasSufficientMemory  kubelet has sufficient memory available
  DiskPressure          False   Sun, 25 Jun 2017 01:39:38 +0000         Tue, 20 Jun 2017 19:27:31 +0000         KubeletHasNoDiskPressure    kubelet has no disk pressure
  Ready                 True    Sun, 25 Jun 2017 01:39:38 +0000         Fri, 23 Jun 2017 17:31:54 +0000         KubeletReady                        kubelet is posting ready status
Addresses:              172.31.120.16,172.31.120.16,centos-minion1
Capacity:
 alpha.kubernetes.io/nvidia-gpu:        0
 cpu:                                   1
 memory:                                1015348Ki
 pods:                                  110
Allocatable:
 alpha.kubernetes.io/nvidia-gpu:        0
 cpu:                                   1
 memory:                                1015348Ki
 pods:                                  110
System Info:
 Machine ID:                    f9afeb75a5a382dce8269887a67fbf58
 System UUID:                   EC2C8A0E-91D6-F54E-5A49-534A6A903FDA
 Boot ID:                       20961efd-c946-481a-97cb-7788209551ae
 Kernel Version:                3.10.0-327.28.2.el7.x86_64
 OS Image:                      CentOS Linux 7 (Core)
 Operating System:              linux
 Architecture:                  amd64
 Container Runtime Version:     docker://1.12.6
 Kubelet Version:               v1.5.2
 Kube-Proxy Version:            v1.5.2
ExternalID:                     centos-minion1
Non-terminated Pods:            (0 in total)
  Namespace                     Name            CPU Requests    CPU Limits      Memory Requests Memory Limits</p>

<hr />

<p>Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.
  CPU Requests  CPU Limits      Memory Requests Memory Limits</p>

<hr />

<p>  0 (0%)        0 (0%)          0 (0%)          0 (0%)
Events:
  FirstSeen     LastSeen        Count   From                            SubObjectPath   Type            Reason             Message</p>

<hr />

<p>  15m           15m             1       {kubelet centos-minion1}                        Normal          Starting           Starting kubelet.
  15m           15m             1       {kubelet centos-minion1}                        Warning         ImageGCFailed      unable to find data for container /
  15m           15m             2       {kubelet centos-minion1}                        Normal          NodeHasSufficientDisk       Node centos-minion1 status is now: NodeHasSufficientDisk
  15m           15m             2       {kubelet centos-minion1}                        Normal          NodeHasSufficientMemory     Node centos-minion1 status is now: NodeHasSufficientMemory
  15m           15m             2       {kubelet centos-minion1}                        Normal          NodeHasNoDiskPressure       Node centos-minion1 status is now: NodeHasNoDiskPressure
  15m           15m             1       {kubelet centos-minion1}                        Warning         Rebooted           Node centos-minion1 has been rebooted, boot id: 20961efd-c946-481a-97cb-7788209551ae
```</p>

<p><strong>O</strong>bviamente recebemos um retorno com muitas informações em formato Json, o que nem sempre é como esperamos. Existem formas de filtrar os resultados e conseguir informações mais precisas, como o bom e velho grep:</p>

<p><code>
[root@kalib1 ~]# kubectl describe node centos-minion1 | grep Addresses
Addresses:              172.31.120.16,172.31.120.16,centos-minion1
</code></p>

<p><strong>V</strong>ocê também pode utilizar expressões regulares e a sintaxe do próprio Kubernetes para consultas mais complexas, como por exemplo, formatar a minha saída Json de forma a pegar apenas a listagem de status dos meus nodes que estão com Ready = True:</p>

<p>```
[root@kalib1 ~]# kubectl get nodes -o jsonpath=&lsquo;{range .items[<em>]}{@.metadata.name}:{range @.status.conditions[</em>]}{@.type}={@.status};{end}{end}&rsquo;| tr &lsquo;;&rsquo; &ldquo;\n&rdquo; | grep &ldquo;Ready=True&rdquo;</p>

<p>Ready=True
Ready=True
Ready=True
```</p>

<p><strong>A</strong> sua criatividade é o limite. ;]</p>

<p><strong>N</strong>ão temos nenhum pod configurado, mas também poderíamos utilizar <em>kubectl get</em> para conseguir a listagem de nossos pods:</p>

<p>```
[root@kalib1 ~]# kubectl get pods</p>

<p>No resources found.
```</p>

<h2>Criando pods</h2>

<p><strong>A</strong>ssim como com o Docker, Ansible e algumas outras ferramentas, utilizaremos a linguagem <a href="https://pt.wikipedia.org/wiki/YAML">YAML</a> para criar nossos arquivos de configuração.</p>

<p><strong>C</strong>riaremos um diretório chamado <em>Builds</em> em nosso Master Controller apenas para melhor organizar nossos arquivos de configuração e ficar mais fácil encontrá-los no futuro:</p>

<p>```</p>

<h1>mkdir Builds</h1>

<h1>cd Builds</h1>

<p>```</p>

<p><strong>P</strong>ara criarmos Pods, o que fazemos na verdade é criar arquivos de configuração que vão dizer ao Kubernetes qual o estado em que desejamos nossa infraestrutura. O papel do Kubernetes é ler esta configuração e assegurar que o estado de nossa infraestrutura reflita o estado desejado.</p>

<p><strong>P</strong>ara facilitar, vamos utilizar exemplos encontrados na própria documentação do Kubernetes. Comecemos com a criação de um Pod para um servidor web Nginx.</p>

<p><strong>V</strong>amos criar um arquivo chamado nginx.yaml dentro do diretório Builds que criamos anteriormente:</p>

<p>```</p>

<h1>vim nginx.yaml</h1>

<p>```</p>

<p><strong>N</strong>o arquivo indicaremos alguns atributos ou variáveis, bem como seus respectivos valores:</p>

<ul>
<li>apiVersion &ndash; Indica a versão da API do kubernetes utilizada</li>
<li>kind &ndash; o tipo de recurso que desejamos</li>
<li>metadata &ndash; dados referentes ao recurso desejado</li>
<li>spec &ndash; especificações sobre o que este recurso irá conter</li>
</ul>


<p><strong>V</strong>amos criar um Pod contendo um único container rodando a versão 1.7.9 do nginx bem como disponibilizando a porta 80 para receber conexões. Este deverá ser o conteúdo do arquivo <em>nginx.yaml</em>:</p>

<p>```
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:</p>

<pre><code>- name: nginx
  image: nginx:1.7.9
  ports:
  - containerPort: 80
</code></pre>

<p>```</p>

<p><strong>A</strong>ntes de executarmos, vamos nos certificar novamente de duas coisas:</p>

<ul>
<li>Que realmente não temos nenhum Pod criado e ativo;</li>
<li>Que não temos nenhum container rodando em nossos nodes.</li>
</ul>


<p><em>No centos-master:</em></p>

<p>```
[root@kalib1 Builds]# kubectl get pods</p>

<p>No resources found.
```</p>

<p><em>No centos-minion1: (Execute o mesmo comando nos demais nodes (centos-minion2 e centos-minion3))</em></p>

<p>```
[root@kalib2 ~]# docker ps</p>

<p>CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
```</p>

<p><em>Novamente: Se você não faz ideia do que acabei de digitar, (docker ps) volte e leia um pouco sobre <a href="/blog/2015/08/20/docker-uma-alternativa-elegante-para-containers-no-linux/">Docker</a> antes de seguir com este artigo.</em></p>

<p><strong>C</strong>omo podemos ver, não temos nenhum Pod, bem como nenhum container rodando em nossos nodes.</p>

<p><strong>V</strong>amos utilizar <em>kubectl create</em> para criar o Pod utilizando o arquivo que criamos <em>nginx.yaml</em>: <em>Executaremos este comando no Master Controller &ndash; centos-master</em></p>

<p>```
[root@kalib1 Builds]# kubectl create -f nginx.yaml</p>

<p>pod &ldquo;nginx&rdquo; created
```</p>

<p><strong>O</strong> Kubernetes está dizendo que nosso Pod &ldquo;nginx&rdquo; foi criado. Vamos verificar:</p>

<p>```
[root@kalib1 Builds]# kubectl get pods</p>

<p>NAME      READY     STATUS    RESTARTS   AGE
nginx     1/1       Running   0          1m
```</p>

<p><strong>O</strong> pod está criado e rodando. Agora, execute novamente <em>docker ps</em> nos 3 nodes para identificar em qual deles o container foi criado. Sim, como não especificamos nada, o Kubernetes vai verificar os recursos disponíveis no momento e vai lançar onde ele achar mais adequado.</p>

<p>```
[root@kalib4 ~]# docker ps</p>

<p>CONTAINER ID        IMAGE                                      COMMAND                  CREATED             STATUS              PORTS               NAMES
6de8e22e1536        nginx:1.7.9                                &ldquo;nginx -g &lsquo;daemon off&rdquo;   2 minutes ago       Up 2 minutes                            k8s_nginx.b0df00ef_nginx_default_d4debd3a-594c-11e7-b587-06827a5b32d4_583881e0
ae49b36ae11b        gcr.io/google_containers/pause-amd64:3.0   &ldquo;/pause&rdquo;                 2 minutes ago       Up 2 minutes                            k8s_POD.b2390301_nginx_default_d4debd3a-594c-11e7-b587-06827a5b32d4_fb5c834f
```</p>

<p><strong>S</strong>im, existem dois containers rodando. Um deles é o nosso &ldquo;nginx&rdquo;, enquanto que o outro é um container padrão do google chamado &ldquo;/pause&rdquo;, o qual será responsável pela manutenção de alguns recursos de nosso cluster.</p>

<p><strong>P</strong>odemos novamente pedir a descrição deste pod que acabamos de criar:</p>

<p>```
[root@kalib1 Builds]# kubectl describe pod nginx</p>

<p>Name:           nginx
Namespace:      default
Node:           centos-minion3/172.31.123.22
Start Time:     Sun, 25 Jun 2017 02:20:18 +0000
Labels:         <none>
Status:         Running
IP:             172.17.0.2
Controllers:    <none>
Containers:
  nginx:</p>

<pre><code>Container ID:               docker://6de8e22e153618271bb6e8095c68070126541331c8acfc3f5d1a654f4b978454
Image:                      nginx:1.7.9
Image ID:                   docker-pullable://docker.io/nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451
Port:                       80/TCP
State:                      Running
  Started:                  Sun, 25 Jun 2017 02:20:27 +0000
Ready:                      True
Restart Count:              0
Volume Mounts:              &lt;none&gt;
Environment Variables:      &lt;none&gt;
</code></pre>

<p>Conditions:
  Type          Status
  Initialized   True
  Ready         True
  PodScheduled  True
No volumes.
QoS Class:      BestEffort
Tolerations:    <none>
Events:
  FirstSeen     LastSeen        Count   From                            SubObjectPath           Type            Reason     Message</p>

<hr />

<p>  6m            6m              1       {default-scheduler }                                    Normal          Scheduled  Successfully assigned nginx to centos-minion3
  6m            6m              1       {kubelet centos-minion3}        spec.containers{nginx}  Normal          Pulling    pulling image &ldquo;nginx:1.7.9&rdquo;
  6m            6m              2       {kubelet centos-minion3}                                Warning         MissingClusterDNS   kubelet does not have ClusterDNS IP configured and cannot create Pod using &ldquo;ClusterFirst&rdquo; policy. Falling back to DNSDefault policy.
  6m            6m              1       {kubelet centos-minion3}        spec.containers{nginx}  Normal          Pulled     Successfully pulled image &ldquo;nginx:1.7.9&rdquo;
  6m            6m              1       {kubelet centos-minion3}        spec.containers{nginx}  Normal          Created    Created container with docker id 6de8e22e1536; Security:[seccomp=unconfined]
  6m            6m              1       {kubelet centos-minion3}        spec.containers{nginx}  Normal          Started    Started container with docker id 6de8e22e1536
```</p>

<p><strong>O</strong>bviamente que isto é apenas a configuração mais básica que se possa imaginar, sem storage, mapeamentos de portas, redirecionamentos, rotas, etc. A ideia é apenas uma apresentação inicial..o que é o Kubernetes.</p>

<p><strong>H</strong>appy Hacking</p>
]]></content>
  </entry>
  
</feed>
